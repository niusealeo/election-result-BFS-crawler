{
  "info": {
    "_postman_id": "68affbb6-d3a2-43f9-b96d-f945eea51e7d",
    "name": "NZ Elections",
    "description": "This Breadth First Search Crawler requires an initial seed url in a Postman collections runner commpatible json format to begin. This crawler collects urls and identifies several data file types.\n\n[urls-level-1.json](https://github.com/niusealeo/election-result-BFS-crawler/blob/main/BFS_crawl/_meta/electionresults.govt.nz/artifacts/urls-level-1.json) is available to use as an example of a top level domain to begin searching the website: [https://www.electionresults.govt.nz/](https://www.electionresults.govt.nz/) , but any website url could be used instead.\n\nThis Postman collection runner \"API\" requires to be run locally on a Postman desktop app so it can work side-by-side with a locally served \"Sink\" file download manager in the Git repo here: [https://github.com/niusealeo/election-result-BFS-crawler](https://github.com/niusealeo/election-result-BFS-crawler)\n\nTo begin the first collection run, select the \"Runs\" option, choose only \"Discover Links\", upload the urls-level-1.json seed file into the test data, and then run the NZ Elections collection.\n\nA next level urls-level-2.json should have have been issued by the \"Sink\" into a /[BFS_crawl](https://github.com/niusealeo/election-result-BFS-crawler/tree/main/BFS_crawl)/[_meta](https://github.com/niusealeo/election-result-BFS-crawler/tree/main/BFS_crawl/_meta)/[electionresults.govt.nz](https://github.com/niusealeo/election-result-BFS-crawler/tree/main/BFS_crawl/_meta/electionresults.govt.nz)/[artifacts](https://github.com/niusealeo/election-result-BFS-crawler/tree/main/BFS_crawl/_meta/electionresults.govt.nz/artifacts)/ directory. If the static html page returned by the seed url also contained file links with extensions xls, xlsx, csv, txt, pdf, zip, etc. then a files-level-1.json should have been produced (but not in the case of the [https://www.electionresults.govt.nz/](https://www.electionresults.govt.nz/) landing page which doesn't have file urls).\n\nThe collections run can then be manually repeated with each subsequent urls-level-X.json, until satisifed. The BFS collections would eventually return all the urls in the static html of an entire \"closed\" website system.\n\n\"Get Electorate Enumeration\" performs a similar task, but lists the official and alphabetic order of the electorates in each MMP election. [Run these urls](https://github.com/niusealeo/election-result-BFS-crawler/blob/main/BFS_crawl/_meta/electionresults.govt.nz/electorate-map-urls.json) in the test data to get the electorate lists for 1996-2023.\n\nThe lists of files-level-X.json can then also be downloaded into the Sink downloads directory using the same steps with the Postman \"Download Files\" collection runner. All the files are hashed into an index to ensure no identical files are downloaded even if they have different names or have been stored in multiple places on the website.\n\n\"Sink\" organises downloaded files into a domain-scoped folder tree. Election-specific organisation rules are optional. \"Sink\" might be capable of receiving data files from any website, though they would likely be dumped directly into the downloads folder with minimal organisation.\n\nThis BFS Crawler should be used on trusted websites to ensure that safe files are downloaded.",
    "schema": "https://schema.getpostman.com/json/collection/v2.1.0/collection.json",
    "_exporter_id": "2650823"
  },
  "item": [
    {
      "name": "Step 0. Get Electorate Enumeration",
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "const html = pm.response.text();\r",
              "const pageUrl = pm.request.url.toString();\r",
              "\r",
              "const YEAR_TO_TERM = { 1996:45, 1999:46, 2002:47, 2005:48, 2008:49, 2011:50, 2014:51, 2017:52, 2020:53, 2023:54 };\r",
              "\r",
              "const ym = pageUrl.match(/electionresults_(\\d{4})/i);\r",
              "if (!ym) { console.log(\"No GE year in URL\", pageUrl); return; }\r",
              "\r",
              "const geYear = parseInt(ym[1], 10);\r",
              "const term = YEAR_TO_TERM[geYear];\r",
              "const termKey = `term_${term}_(${geYear})`;\r",
              "\r",
              "function decodeHtmlEntities(s) {\r",
              "  s = String(s || \"\");\r",
              "\r",
              "  // hex numeric: &#x14D;\r",
              "  s = s.replace(/&#x([0-9a-fA-F]+);/g, (_, hex) => {\r",
              "    const cp = parseInt(hex, 16);\r",
              "    if (!Number.isFinite(cp)) return _;\r",
              "    try { return String.fromCodePoint(cp); } catch { return _; }\r",
              "  });\r",
              "\r",
              "  // decimal numeric: &#333;\r",
              "  s = s.replace(/&#(\\d+);/g, (_, dec) => {\r",
              "    const cp = parseInt(dec, 10);\r",
              "    if (!Number.isFinite(cp)) return _;\r",
              "    try { return String.fromCodePoint(cp); } catch { return _; }\r",
              "  });\r",
              "\r",
              "  // named entities we actually see\r",
              "  const named = {\r",
              "    amp: \"&\",\r",
              "    lt: \"<\",\r",
              "    gt: \">\",\r",
              "    quot: \"\\\"\",\r",
              "    apos: \"'\",\r",
              "    nbsp: \" \",\r",
              "  };\r",
              "  s = s.replace(/&([a-zA-Z]+);/g, (m, name) => {\r",
              "    const k = String(name).toLowerCase();\r",
              "    return Object.prototype.hasOwnProperty.call(named, k) ? named[k] : m;\r",
              "  });\r",
              "\r",
              "  try { s = s.normalize(\"NFC\"); } catch {}\r",
              "  return s;\r",
              "}\r",
              "\r",
              "function cleanName(s) {\r",
              "  return decodeHtmlEntities(String(s || \"\"))\r",
              "    .replace(/^\\^+\\s*/, \"\")\r",
              "    .replace(/\\s*\\(\\.pdf[^)]*\\)\\s*$/i, \"\")\r",
              "    .replace(/\\s+/g, \" \")\r",
              "    .trim();\r",
              "}\r",
              "\r",
              "function absUrl(href) {\r",
              "  try { return new URL(href, pageUrl).toString(); } catch { return href; }\r",
              "}\r",
              "\r",
              "let pairs = []; // {num, name}\r",
              "\r",
              "// ---------------------- (1) Terms 52+ (2017/2020/2023): electorate-details-N.html ----------------------\r",
              "if (geYear >= 2017) {\r",
              "  const re = /href=[\"'][^\"']*electorate-details-(\\d+)[^\"']*\\.html[^\"']*[\"'][^>]*>\\s*([^<]+?)\\s*<\\/a>/gi;\r",
              "  let m;\r",
              "  while ((m = re.exec(html)) !== null) {\r",
              "    const num = parseInt(m[1], 10);\r",
              "    const name = cleanName(m[2]);\r",
              "    if (Number.isFinite(num) && name) pairs.push({ num, name });\r",
              "  }\r",
              "}\r",
              "\r",
              "// ---------------------- (2) Terms 47–51 (2002–2014): electorate-N(.html / -notable.html etc) ----------------------\r",
              "// This is the one that was wrong before: these years use pages like electorate-2.html etc.\r",
              "if (pairs.length === 0 && (geYear === 2002 || geYear === 2005 || geYear === 2008 || geYear === 2011 || geYear === 2014)) {\r",
              "\r",
              "  // 1) Match <a ... href=\"...electorate-12-notable.html\"...>  NAME (may contain tags) </a>\r",
              "  //    Capture:\r",
              "  //      group 1 = electorate number from href\r",
              "  //      group 2 = inner HTML of the anchor (printed name, possibly with nested tags)\r",
              "  const re = /<a\\b[^>]*href=[\"'][^\"']*electorate-(\\d+)(?:-[^\"']+)?\\.html[^\"']*[\"'][^>]*>([\\s\\S]*?)<\\/a>/gi;\r",
              "\r",
              "  let m;\r",
              "  while ((m = re.exec(html)) !== null) {\r",
              "    const num = parseInt(m[1], 10);\r",
              "\r",
              "    // Strip nested tags from anchor innerHTML and normalize whitespace\r",
              "    let name = decodeHtmlEntities(m[2])\r",
              "  .replace(/<[^>]+>/g, \" \")\r",
              "  .replace(/\\s+/g, \" \")\r",
              "  .trim();\r",
              "\r",
              "    // Remove caret markers some index pages include\r",
              "    name = name.replace(/^\\^+\\s*/, \"\");\r",
              "\r",
              "    if (Number.isFinite(num) && name) {\r",
              "      pairs.push({ num, name });\r",
              "    }\r",
              "  }\r",
              "}\r",
              "\r",
              "\r",
              "// ---------------------- (3) Term 46 (1999): e9_partVIII.html numbered list with links ----------------------\r",
              "if (pairs.length === 0 && geYear === 1999) {\r",
              "  // Pull only the Candidate Vote Details section when possible (reduces false hits)\r",
              "  const start = html.indexOf(\"Candidate Vote Details\");\r",
              "  const end = html.indexOf(\"Party Vote Details\");\r",
              "  const section = (start >= 0 && end > start) ? html.slice(start, end) : html;\r",
              "\r",
              "  // Pattern: \"27 <a ...>Mt Albert</a>\"\r",
              "  const re = /(?:^|>|\\n)\\s*(\\d{1,3})\\s*(?:<\\/?\\w+[^>]*>\\s*)*<a[^>]*>\\s*([^<]+?)\\s*<\\/a>/gi;\r",
              "  let m;\r",
              "  while ((m = re.exec(section)) !== null) {\r",
              "    const num = parseInt(m[1], 10);\r",
              "    const name = cleanName(m[2]);\r",
              "    if (Number.isFinite(num) && name) pairs.push({ num, name });\r",
              "  }\r",
              "}\r",
              "\r",
              "// ---------------------- (4) Term 45 (1996): pollingplaces.html PDF links end with \" NN.pdf\" ----------------------\r",
              "if (pairs.length === 0 && geYear === 1996) {\r",
              "  // Look for PDF anchors; electorate number is usually in the filename \"...%2001.pdf\"\r",
              "  const re = /href=[\"']([^\"']+\\.pdf)[\"'][^>]*>\\s*([^<]+?)\\s*<\\/a>/gi;\r",
              "  let m;\r",
              "  while ((m = re.exec(html)) !== null) {\r",
              "    const hrefAbs = absUrl(m[1]);\r",
              "    const name = cleanName(m[2]);\r",
              "\r",
              "    // Extract trailing 2-digit electorate number from pdf filename\r",
              "    const numMatch = decodeURIComponent(hrefAbs).match(/(?:\\s|%20)(\\d{2})\\.pdf$/i);\r",
              "    const num = numMatch ? parseInt(numMatch[1], 10) : null;\r",
              "\r",
              "    if (name && Number.isFinite(num)) pairs.push({ num, name });\r",
              "  }\r",
              "}\r",
              "\r",
              "// -------- Build official_order (num -> name), de-dup by num ----------\r",
              "const byNum = new Map();\r",
              "for (const p of pairs) if (!byNum.has(p.num)) byNum.set(p.num, p.name);\r",
              "\r",
              "const nums = [...byNum.keys()].sort((a,b)=>a-b);\r",
              "if (nums.length === 0) {\r",
              "  console.log(\"NO ELECTORATES PARSED:\", termKey, pageUrl);\r",
              "  return;\r",
              "}\r",
              "\r",
              "const official_order = {};\r",
              "for (const n of nums) official_order[String(n)] = byNum.get(n);\r",
              "\r",
              "// Alpha index (name -> alpha index)\r",
              "const names = Object.values(official_order);\r",
              "const alpha = [...names].sort((a,b)=>a.localeCompare(b, \"en\", { sensitivity: \"base\" }));\r",
              "const alphabetical_order = {};\r",
              "alpha.forEach((nm,i)=>alphabetical_order[nm]=i+1);\r",
              "\r",
              "console.log(\"Parsed\", termKey, \"count\", nums.length);\r",
              "\r",
              "// Post to sink (sink merges term-by-term)\r",
              "pm.sendRequest({\r",
              "  url: \"http://localhost:3000/meta/electorates\",\r",
              "  method: \"POST\",\r",
              "  header: { \"Content-Type\": \"application/json\" },\r",
              "  body: { mode: \"raw\", raw: JSON.stringify({ termKey, official_order, alphabetical_order }) }\r",
              "}, (err, res) => {\r",
              "  if (err) console.log(\"meta upload error\", err);\r",
              "  else console.log(\"meta upload ok\", res.code, termKey);\r",
              "});\r",
              ""
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        },
        {
          "listen": "prerequest",
          "script": {
            "exec": [
              ""
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        }
      ],
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "{{url}}",
          "host": [
            "{{url}}"
          ]
        }
      },
      "response": []
    },
    {
      "name": "Step 1. Discover Links (Legacy)",
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "// === Discover Links — Post-response (run-isolated; no carry-over between runs) ===\r",
              "// - Clears DISCOVER_* scratch vars at start of each collection runner execution.\r",
              "// - Meta row may be real URL row: it stores level/kind but still parses links.\r",
              "// - Accumulates pages/files/visited for THIS run only.\r",
              "// - Sends ONE payload to sink at end-of-run.\r",
              "\r",
              "// ---------- Run boundary: clear scratch vars on iteration 0 ----------\r",
              "if (pm.info.iteration === 0) {\r",
              "  pm.collectionVariables.unset(\"DISCOVER_pages\");\r",
              "  pm.collectionVariables.unset(\"DISCOVER_visited\");\r",
              "  pm.collectionVariables.unset(\"DISCOVER_fileMap\");\r",
              "  pm.collectionVariables.unset(\"DISCOVER_level\");\r",
              "  pm.collectionVariables.unset(\"DISCOVER_kind\");\r",
              "  console.log(\"Discover Links: cleared DISCOVER_* scratch vars for new run\");\r",
              "}\r",
              "\r",
              "// ---------- Helpers ----------\r",
              "function loadSet(name) {\r",
              "  return new Set(JSON.parse(pm.collectionVariables.get(name) || \"[]\"));\r",
              "}\r",
              "function saveSet(name, set) {\r",
              "  pm.collectionVariables.set(name, JSON.stringify(Array.from(set), null, 2));\r",
              "}\r",
              "function loadObj(name) {\r",
              "  return JSON.parse(pm.collectionVariables.get(name) || \"{}\");\r",
              "}\r",
              "function saveObj(name, obj) {\r",
              "  pm.collectionVariables.set(name, JSON.stringify(obj, null, 2));\r",
              "}\r",
              "\r",
              "// ---------- Meta row may be REAL url row ----------\r",
              "const currentUrl = pm.request.url.toString();\r",
              "\r",
              "if (pm.iterationData.get(\"_meta\") === true) {\r",
              "  // store for later rows, but DO NOT skip discovery\r",
              "  pm.collectionVariables.set(\"DISCOVER_level\", String(pm.iterationData.get(\"level\") || 1));\r",
              "  pm.collectionVariables.set(\"DISCOVER_kind\", String(pm.iterationData.get(\"kind\") || \"urls\"));\r",
              "  console.log(\"Discover Links: meta stored (conflated real row)\");\r",
              "}\r",
              "\r",
              "const level = Number(pm.iterationData.get(\"level\") || pm.collectionVariables.get(\"DISCOVER_level\") || 1);\r",
              "const kind = String(pm.iterationData.get(\"kind\") || pm.collectionVariables.get(\"DISCOVER_kind\") || \"urls\");\r",
              "\r",
              "if (kind !== \"urls\") {\r",
              "  console.log(\"Discover Links: skipping because kind != urls\");\r",
              "  return;\r",
              "}\r",
              "\r",
              "const html = pm.response.text();\r",
              "const origin = new URL(currentUrl).origin;\r",
              "\r",
              "// ---------- Filters ----------\r",
              "function isSkippableHref(href) {\r",
              "  if (!href) return true;\r",
              "  const h = href.trim();\r",
              "  if (!h) return true;\r",
              "  return (\r",
              "    h.startsWith(\"#\") ||\r",
              "    h.startsWith(\"javascript:\") ||\r",
              "    h.startsWith(\"mailto:\") ||\r",
              "    h.startsWith(\"tel:\")\r",
              "  );\r",
              "}\r",
              "\r",
              "function normalizeUrl(u) {\r",
              "  if (!u) return u;\r",
              "  // Fix common HTML-entity leakage in query strings (&amp;... or repeated amp;)\r",
              "  let cleaned = String(u).replace(/amp;/g, \"\");\r",
              "  try { return new URL(cleaned).toString(); } catch { return cleaned; }\r",
              "}\r",
              "\r",
              "function toAbs(href) {\r",
              "  try { return normalizeUrl(new URL(href, currentUrl).toString()); } catch { return null; }\r",
              "}\r",
              "\r",
              "function isJunkAssetUrl(u) {\r",
              "  return /\\.(css|js|mjs|map|png|jpg|jpeg|gif|webp|svg|ico|woff2?|ttf|eot|otf)(\\?|#|$)/i.test(u) ||\r",
              "         /\\/(assets|static|images|img|fonts)\\//i.test(u) ||\r",
              "         /\\/(robots\\.txt|sitemap\\.xml)(\\?|#|$)/i.test(u);\r",
              "}\r",
              "\r",
              "// function isElectionFile(u) {\r",
              "//   return /\\.(csv|pdf|xlsx|xls|txt|zip)(\\?|#|$)/i.test(u);\r",
              "// }\r",
              "\r",
              "function isDownloadableFile(u) {\r",
              "    // Common information-site document/data archives\r",
              "    // NOTE: this is URL-extension based; for extensionless URLs use the Probe step (HEAD/Range) to classify by Content-Type.\r",
              "    return /\\.(csv|tsv|pdf|xlsx|xls|ods|odt|odp|pptx|ppt|docx|doc|rtf|txt|md|epub|zip|7z|tar|gz|tgz|bz2|xz|rar|brf|mp3|m4a|wav|ogg|flac)(\\?|#|$)/i.test(u);\r",
              "}\r",
              "\r",
              "function extOf(u) {\r",
              "  const m = u.match(/\\.([a-z0-9]+)(?:\\?|#|$)/i);\r",
              "  return m ? m[1].toLowerCase() : null;\r",
              "}\r",
              "\r",
              "// ---------- Load per-run accumulators ----------\r",
              "const pageSet = loadSet(\"DISCOVER_pages\");\r",
              "const visitedSet = loadSet(\"DISCOVER_visited\");\r",
              "\r",
              "// BFS identity for files = file URL; keep best metadata\r",
              "const fileMap = loadObj(\"DISCOVER_fileMap\");\r",
              "\r",
              "visitedSet.add(currentUrl);\r",
              "\r",
              "// ---------- Extract hrefs ----------\r",
              "const hrefRe = /href\\s*=\\s*[\"']([^\"']+)[\"']/gi;\r",
              "let m;\r",
              "let newPages = 0;\r",
              "let newFiles = 0;\r",
              "\r",
              "while ((m = hrefRe.exec(html)) !== null) {\r",
              "  const raw = (m[1] || \"\").trim();\r",
              "  if (isSkippableHref(raw)) continue;\r",
              "\r",
              "  const abs = toAbs(raw);\r",
              "  if (!abs) continue;\r",
              "\r",
              "  if (!abs.startsWith(origin)) continue;\r",
              "  if (isJunkAssetUrl(abs)) continue;\r",
              "\r",
              "  if (isDownloadableFile(abs)) {\r",
              "    const ext = extOf(abs) || \"bin\";\r",
              "    const existing = fileMap[abs];\r",
              "\r",
              "    if (!existing) {\r",
              "      fileMap[abs] = { url: abs, ext, source_page_url: currentUrl };\r",
              "      newFiles++;\r",
              "    } else {\r",
              "      // upgrade metadata without duplicating identity\r",
              "      if (!existing.source_page_url) existing.source_page_url = currentUrl;\r",
              "      if ((!existing.ext || existing.ext === \"bin\") && ext !== \"bin\") existing.ext = ext;\r",
              "      fileMap[abs] = existing;\r",
              "    }\r",
              "    continue;\r",
              "  }\r",
              "\r",
              "  if (!pageSet.has(abs)) { pageSet.add(abs); newPages++; }\r",
              "}\r",
              "\r",
              "// ---------- Persist accumulators ----------\r",
              "saveSet(\"DISCOVER_pages\", pageSet);\r",
              "saveSet(\"DISCOVER_visited\", visitedSet);\r",
              "saveObj(\"DISCOVER_fileMap\", fileMap);\r",
              "\r",
              "console.log(\"DISCOVER:\", currentUrl);\r",
              "console.log(\"New pages:\", newPages, \"New files:\", newFiles);\r",
              "console.log(\"Totals pages:\", pageSet.size, \"Totals files:\", Object.keys(fileMap).length);\r",
              "\r",
              "// ---------- End-of-run: send ONE payload ----------\r",
              "const isLast = pm.info.iteration === (pm.info.iterationCount - 1);\r",
              "if (isLast) {\r",
              "  const pagesOut = Array.from(pageSet).map(url => ({ url }));\r",
              "  const visitedOut = Array.from(visitedSet).map(url => ({ url }));\r",
              "  const filesOut = Object.values(fileMap);\r",
              "\r",
              "  pm.sendRequest({\r",
              "    url: \"http://localhost:3000/dedupe/level\",\r",
              "    method: \"POST\",\r",
              "    header: { \"Content-Type\": \"application/json\" },\r",
              "    body: {\r",
              "      mode: \"raw\",\r",
              "      raw: JSON.stringify({\r",
              "        level: level,\r",
              "        visited: visitedOut,\r",
              "        pages: pagesOut,\r",
              "        ...(filesOut.length ? { files: filesOut } : {})\r",
              "      })\r",
              "    }\r",
              "  }, (err, res) => {\r",
              "    if (err) console.log(\"sink /dedupe/level ERROR:\", err);\r",
              "    else console.log(\"sink /dedupe/level OK:\", res.code);\r",
              "  });\r",
              "\r",
              "  // Optional: clear after send as well (belt + braces)\r",
              "  // pm.collectionVariables.unset(\"DISCOVER_pages\");\r",
              "  // pm.collectionVariables.unset(\"DISCOVER_visited\");\r",
              "  // pm.collectionVariables.unset(\"DISCOVER_fileMap\");\r",
              "  // pm.collectionVariables.unset(\"DISCOVER_level\");\r",
              "  // pm.collectionVariables.unset(\"DISCOVER_kind\");\r",
              "}\r",
              "\r"
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        },
        {
          "listen": "prerequest",
          "script": {
            "exec": [
              "// === Discover Links — Pre-request ===\r",
              "// Uses conflated meta-with-first-row: first row is real url and also has _meta.\r",
              "// Resets sets once per run file (on meta row).\r",
              "\r",
              "if (pm.iterationData.get(\"_meta\") === true) {\r",
              "  const lvl = Number(pm.iterationData.get(\"level\") || 1);\r",
              "  pm.collectionVariables.set(\"DISCOVER_LEVEL\", String(lvl));\r",
              "\r",
              "  // reset per-level accumulators\r",
              "  [\"discoveredLinks\",\"csvFiles\",\"pdfFiles\",\"xlsxFiles\",\"txtFiles\",\"zipFiles\"].forEach(k =>\r",
              "    pm.collectionVariables.set(k, \"[]\")\r",
              "  );\r",
              "\r",
              "  console.log(\"DISCOVER init/reset for level\", lvl);\r",
              "}\r",
              "\r",
              "\r",
              "\r",
              "// Set request URL from iteration data when running in Collection Runner\r",
              "const rowUrl = pm.iterationData.get(\"url\") || pm.variables.get(\"url\");\r",
              "if (rowUrl) { pm.request.url = rowUrl; }\r"
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        }
      ],
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "{{url}}"
        }
      },
      "response": []
    },
    {
      "name": "Step 1. Discover Links (Large Lists)",
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "// === Discover Links — Post-response (streaming; memory-safe for huge levels) ===\r",
              "// Instead of storing growing Sets/Maps in Postman variables (which can blow up on 10k+ URLs),\r",
              "// this step sends small batches to the sink, which dedupes and finalizes once at end-of-run.\r",
              "\r",
              "const currentUrl = pm.request.url.toString();\r",
              "\r",
              "// Robust sink base resolution\r",
              "const sinkBase =\r",
              "  pm.collectionVariables.get(\"SINK_BASE\") ||\r",
              "  pm.environment.get(\"SINK_BASE\") ||\r",
              "  pm.globals.get(\"SINK_BASE\") ||\r",
              "  \"http://localhost:3000\";\r",
              "\r",
              "if (pm.iterationData.get(\"_meta\") === true) {\r",
              "  pm.collectionVariables.set(\"DISCOVER_level\", String(pm.iterationData.get(\"level\") || 1));\r",
              "  pm.collectionVariables.set(\"DISCOVER_kind\", String(pm.iterationData.get(\"kind\") || \"urls\"));\r",
              "}\r",
              "\r",
              "const level = Number(pm.iterationData.get(\"level\") || pm.collectionVariables.get(\"DISCOVER_level\") || 1);\r",
              "const kind = String(pm.iterationData.get(\"kind\") || pm.collectionVariables.get(\"DISCOVER_kind\") || \"urls\");\r",
              "if (kind !== \"urls\") return;\r",
              "\r",
              "// Run id is created in pre-request and reused for the whole run\r",
              "const runId = pm.collectionVariables.get(\"DISCOVER_RUN_ID\") || \"run_unknown\";\r",
              "\r",
              "const html = pm.response.text();\r",
              "const origin = new URL(currentUrl).origin;\r",
              "\r",
              "function isSkippableHref(href) {\r",
              "  if (!href) return true;\r",
              "  const h = href.trim();\r",
              "  if (!h) return true;\r",
              "  return (\r",
              "    h.startsWith(\"#\") ||\r",
              "    h.startsWith(\"javascript:\") ||\r",
              "    h.startsWith(\"mailto:\") ||\r",
              "    h.startsWith(\"tel:\")\r",
              "  );\r",
              "}\r",
              "function normalizeUrl(u) {\r",
              "  if (!u) return u;\r",
              "  // Fix common HTML-entity leakage in query strings (&amp;... or repeated amp;)\r",
              "  let cleaned = String(u).replace(/amp;/g, \"\");\r",
              "  try { return new URL(cleaned).toString(); } catch { return cleaned; }\r",
              "}\r",
              "\r",
              "function toAbs(href) {\r",
              "  try { return normalizeUrl(new URL(href, currentUrl).toString()); } catch { return null; }\r",
              "}\r",
              "function isJunkAssetUrl(u) {\r",
              "  return /\\.(css|js|mjs|map|png|jpg|jpeg|gif|webp|svg|ico|woff2?|ttf|eot|otf)(\\?|#|$)/i.test(u) ||\r",
              "         /\\/(assets|static|images|img|fonts)\\//i.test(u) ||\r",
              "         /\\/(robots\\.txt|sitemap\\.xml)(\\?|#|$)/i.test(u);\r",
              "}\r",
              "\r",
              "// Generic downloadable extensions (still extension-based; Content-Type probing can be layered later)\r",
              "function isDownloadableFile(u) {\r",
              "  return /\\.(pdf|doc|docx|rtf|odt|epub|ppt|pptx|odp|xls|xlsx|ods|csv|tsv|txt|md|brf|zip|7z|tar|gz|tgz|bz2|xz|rar|mp3|m4a|wav|ogg|flac)(\\?|#|$)/i.test(u);\r",
              "}\r",
              "function extOf(u) {\r",
              "  const m = u.match(/\\.([a-z0-9]+)(?:\\?|#|$)/i);\r",
              "  return m ? m[1].toLowerCase() : null;\r",
              "}\r",
              "\r",
              "// Extract hrefs and build a small delta\r",
              "const hrefRe = /href\\s*=\\s*[\"']([^\"']+)[\"']/gi;\r",
              "let m;\r",
              "const deltaPages = [];\r",
              "const deltaFiles = [];\r",
              "\r",
              "while ((m = hrefRe.exec(html)) !== null) {\r",
              "  const raw = (m[1] || \"\").trim();\r",
              "  if (isSkippableHref(raw)) continue;\r",
              "  const abs = toAbs(raw);\r",
              "  if (!abs) continue;\r",
              "  if (!abs.startsWith(origin)) continue;\r",
              "  if (isJunkAssetUrl(abs)) continue;\r",
              "\r",
              "  if (isDownloadableFile(abs)) {\r",
              "    deltaFiles.push({ url: abs, ext: extOf(abs) || \"bin\", source_page_url: currentUrl });\r",
              "  } else {\r",
              "    deltaPages.push({ url: abs });\r",
              "  }\r",
              "}\r",
              "\r",
              "// Buffer deltas in small batches to avoid spamming the sink\r",
              "function loadArr(k) {\r",
              "  try { return JSON.parse(pm.collectionVariables.get(k) || \"[]\"); } catch { return []; }\r",
              "}\r",
              "function saveArr(k, v) {\r",
              "  pm.collectionVariables.set(k, JSON.stringify(v));\r",
              "}\r",
              "\r",
              "const bufPagesKey = \"DISCOVER_BUF_pages\";\r",
              "const bufFilesKey = \"DISCOVER_BUF_files\";\r",
              "const bufVisitedKey = \"DISCOVER_BUF_visited\";\r",
              "\r",
              "const bufPages = loadArr(bufPagesKey);\r",
              "const bufFiles = loadArr(bufFilesKey);\r",
              "const bufVisited = loadArr(bufVisitedKey);\r",
              "\r",
              "bufVisited.push({ url: currentUrl });\r",
              "for (const p of deltaPages) bufPages.push(p);\r",
              "for (const f of deltaFiles) bufFiles.push(f);\r",
              "\r",
              "saveArr(bufPagesKey, bufPages);\r",
              "saveArr(bufFilesKey, bufFiles);\r",
              "saveArr(bufVisitedKey, bufVisited);\r",
              "\r",
              "const BATCH_TARGET = 50;\r",
              "const isLast = pm.info.iteration === (pm.info.iterationCount - 1);\r",
              "const shouldFlush = isLast || bufVisited.length >= BATCH_TARGET;\r",
              "\r",
              "function flush(cb) {\r",
              "  const v = loadArr(bufVisitedKey);\r",
              "  const p = loadArr(bufPagesKey);\r",
              "  const f = loadArr(bufFilesKey);\r",
              "\r",
              "  if (!v.length && !p.length && !f.length) { if (cb) cb(); return; }\r",
              "\r",
              "  // Clear buffers BEFORE sending to reduce risk of re-send loops\r",
              "  saveArr(bufVisitedKey, []);\r",
              "  saveArr(bufPagesKey, []);\r",
              "  saveArr(bufFilesKey, []);\r",
              "\r",
              "  pm.sendRequest({\r",
              "    url: `${sinkBase}/runs/append/urls`,\r",
              "    method: \"POST\",\r",
              "    header: { \"Content-Type\": \"application/json\" },\r",
              "    body: { mode: \"raw\", raw: JSON.stringify({ level, run_id: runId, visited: v, pages: p, files: f }) }\r",
              "  }, (err, res) => {\r",
              "    if (err) console.log(\"sink /runs/append/urls ERROR:\", err);\r",
              "    else console.log(\"sink /runs/append/urls OK:\", res.code);\r",
              "    if (cb) cb();\r",
              "  });\r",
              "}\r",
              "\r",
              "if (shouldFlush) {\r",
              "  flush(() => {\r",
              "    if (isLast) {\r",
              "      pm.sendRequest({\r",
              "        url: `${sinkBase}/runs/finalize/urls`,\r",
              "        method: \"POST\",\r",
              "        header: { \"Content-Type\": \"application/json\" },\r",
              "        body: { mode: \"raw\", raw: JSON.stringify({ level, run_id: runId }) }\r",
              "      }, (err, res) => {\r",
              "        if (err) console.log(\"sink /runs/finalize/urls ERROR:\", err);\r",
              "        else console.log(\"sink /runs/finalize/urls OK:\", res.code);\r",
              "      });\r",
              "    }\r",
              "  });\r",
              "}\r",
              "\r"
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        },
        {
          "listen": "prerequest",
          "script": {
            "exec": [
              "// === Discover Links — Pre-request ===\r",
              "// Uses conflated meta-with-first-row: first row is real url and also has _meta.\r",
              "// Starts a streaming discovery run on the sink, and initializes small buffers.\r",
              "\r",
              "// Robust sink base resolution\r",
              "const sinkBase =\r",
              "  pm.collectionVariables.get(\"SINK_BASE\") ||\r",
              "  pm.environment.get(\"SINK_BASE\") ||\r",
              "  pm.globals.get(\"SINK_BASE\") ||\r",
              "  \"http://localhost:3000\";\r",
              "\r",
              "if (pm.iterationData.get(\"_meta\") === true) {\r",
              "  const lvl = Number(pm.iterationData.get(\"level\") || 1);\r",
              "  pm.collectionVariables.set(\"DISCOVER_LEVEL\", String(lvl));\r",
              "\r",
              "  // Create/refresh a run id for this Runner execution\r",
              "  const runId = `run_${new Date().toISOString().replace(/[:.]/g,'-')}_${Math.floor(Math.random()*1e9)}`;\r",
              "  pm.collectionVariables.set(\"DISCOVER_RUN_ID\", runId);\r",
              "\r",
              "  // Reset small buffers\r",
              "  pm.collectionVariables.set(\"DISCOVER_BUF_pages\", \"[]\");\r",
              "  pm.collectionVariables.set(\"DISCOVER_BUF_files\", \"[]\");\r",
              "  pm.collectionVariables.set(\"DISCOVER_BUF_visited\", \"[]\");\r",
              "\r",
              "  // Start/reset the sink run bucket\r",
              "  pm.sendRequest({\r",
              "    url: `${sinkBase}/runs/start/urls`,\r",
              "    method: \"POST\",\r",
              "    header: { \"Content-Type\": \"application/json\" },\r",
              "    body: { mode: \"raw\", raw: JSON.stringify({ level: lvl, run_id: runId }) }\r",
              "  }, (err, res) => {\r",
              "    if (err) console.log(\"sink /runs/start/urls ERROR:\", err);\r",
              "    else console.log(\"sink /runs/start/urls OK:\", res.code);\r",
              "  });\r",
              "\r",
              "  console.log(\"DISCOVER streaming init for level\", lvl, \"run\", runId);\r",
              "}\r",
              "\r",
              "\r",
              "\r",
              "// Set request URL from iteration data when running in Collection Runner\r",
              "const rowUrl = pm.iterationData.get(\"url\") || pm.variables.get(\"url\");\r",
              "if (rowUrl) { pm.request.url = rowUrl; }\r"
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        }
      ],
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "{{url}}"
        }
      },
      "response": []
    },
    {
      "name": "Step 2. File Meta Diff Probe (optional)",
      "event": [
        {
          "listen": "prerequest",
          "script": {
            "exec": [
              "const url = pm.iterationData.get(\"url\");\r",
              "if (!url) throw new Error(\"Iteration data must include 'url'\");\r",
              "\r",
              "// Store HEAD meta keyed by URL for this run\r",
              "const key = \"probe_head_meta_\" + encodeURIComponent(url);\r",
              "pm.collectionVariables.unset(key);\r",
              "\r",
              "// Use default Postman headers; no UA spoofing\r",
              "pm.sendRequest(\r",
              "  { url, method: \"HEAD\", timeout: 30000 },\r",
              "  (err, res) => {\r",
              "    let meta;\r",
              "\r",
              "    if (err || !res) {\r",
              "      meta = { ok: false, error: String(err || \"no response\"), status: null, ts: new Date().toISOString() };\r",
              "    } else {\r",
              "      const h = res.headers;\r",
              "      const cl = h.get(\"Content-Length\");\r",
              "      meta = {\r",
              "        ok: res.code < 400,\r",
              "        status: res.code,\r",
              "        etag: h.get(\"ETag\") || null,\r",
              "        last_modified: h.get(\"Last-Modified\") || null,\r",
              "        content_type: h.get(\"Content-Type\") || null,\r",
              "        content_length: (cl && /^\\d+$/.test(cl)) ? Number(cl) : null,\r",
              "        accept_ranges: h.get(\"Accept-Ranges\") || null,\r",
              "        ts: new Date().toISOString()\r",
              "      };\r",
              "    }\r",
              "\r",
              "    pm.collectionVariables.set(key, JSON.stringify(meta));\r",
              "  }\r",
              ");\r",
              ""
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        },
        {
          "listen": "test",
          "script": {
            "exec": [
              "const url = pm.iterationData.get(\"url\");\r",
              "const levelRaw = pm.iterationData.get(\"level\");\r",
              "const level = levelRaw ? Number(levelRaw) : null;\r",
              "\r",
              "if (!url) throw new Error(\"Iteration data must include 'url'\");\r",
              "\r",
              "// Robust sink base resolution\r",
              "const sinkBase =\r",
              "  pm.collectionVariables.get(\"SINK_BASE\") ||\r",
              "  pm.environment.get(\"SINK_BASE\") ||\r",
              "  pm.globals.get(\"SINK_BASE\") ||\r",
              "  \"http://localhost:3000\";\r",
              "\r",
              "const crawlRoot =\r",
              "  pm.collectionVariables.get(\"CRAWL_ROOT\") ||\r",
              "  pm.environment.get(\"CRAWL_ROOT\") ||\r",
              "  pm.globals.get(\"CRAWL_ROOT\") ||\r",
              "  \"BFS_crawl\";\r",
              "\r",
              "// ---- Read stored HEAD meta ----\r",
              "const headKey = \"probe_head_meta_\" + encodeURIComponent(url);\r",
              "let headMeta = null;\r",
              "\r",
              "const headRaw = pm.collectionVariables.get(headKey);\r",
              "if (headRaw) {\r",
              "  try { headMeta = JSON.parse(headRaw); }\r",
              "  catch { headMeta = { ok: false, error: \"bad head meta json\", status: null, ts: new Date().toISOString() }; }\r",
              "} else {\r",
              "  headMeta = { ok: false, error: \"head meta missing (pre-request not completed?)\", status: null, ts: new Date().toISOString() };\r",
              "}\r",
              "\r",
              "// ---- Extract GET Range response meta (visible request) ----\r",
              "const h = pm.response.headers;\r",
              "const contentRange = h.get(\"Content-Range\") || null;\r",
              "const contentLengthHeader = h.get(\"Content-Length\") || null;\r",
              "\r",
              "let totalSize = null;\r",
              "if (contentRange) {\r",
              "  const m = contentRange.match(/\\/(\\d+)\\s*$/);\r",
              "  if (m) totalSize = Number(m[1]);\r",
              "}\r",
              "\r",
              "let contentLength = null;\r",
              "if (Number.isFinite(totalSize)) contentLength = totalSize;\r",
              "else if (contentLengthHeader && /^\\d+$/.test(contentLengthHeader)) contentLength = Number(contentLengthHeader);\r",
              "\r",
              "const getMeta = {\r",
              "  ok: pm.response.code < 400,\r",
              "  status: pm.response.code,\r",
              "  etag: h.get(\"ETag\") || null,\r",
              "  last_modified: h.get(\"Last-Modified\") || null,\r",
              "  content_type: h.get(\"Content-Type\") || null,\r",
              "  content_length: contentLength,\r",
              "  content_range: contentRange,\r",
              "  accept_ranges: h.get(\"Accept-Ranges\") || null,\r",
              "  range_honoured: (pm.response.code === 206) && !!contentRange,\r",
              "  range_ignored: (pm.response.code === 200) && !contentRange,\r",
              "  ts: new Date().toISOString()\r",
              "};\r",
              "\r",
              "// ---- Build combined record ----\r",
              "const record = {\r",
              "  ts: new Date().toISOString(),\r",
              "  crawl_root: crawlRoot,\r",
              "  level,\r",
              "  url,\r",
              "  head: headMeta,\r",
              "  get_range: getMeta\r",
              "};\r",
              "\r",
              "// ---- Ship to sink, but don't crash run if sink is unavailable ----\r",
              "pm.sendRequest(\r",
              "  {\r",
              "    url: `${sinkBase}/probe/meta`,\r",
              "    method: \"POST\",\r",
              "    header: { \"Content-Type\": \"application/json\" },\r",
              "    body: { mode: \"raw\", raw: JSON.stringify(record) },\r",
              "    timeout: 30000\r",
              "  },\r",
              "  (err, res) => {\r",
              "    if (err) {\r",
              "      console.log(`[ProbeMeta] sink POST failed (non-fatal): ${err}`);\r",
              "      return;\r",
              "    }\r",
              "    if (res.code >= 400) {\r",
              "      console.log(`[ProbeMeta] sink rejected meta (non-fatal): ${res.code} ${res.text()}`);\r",
              "    }\r",
              "  }\r",
              ");\r",
              "\r",
              "// Cleanup\r",
              "pm.collectionVariables.unset(headKey);\r",
              ""
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        }
      ],
      "request": {
        "method": "GET",
        "header": [
          {
            "key": "Range",
            "value": "bytes=0-0",
            "type": "text"
          }
        ],
        "url": {
          "raw": "{{url}}",
          "host": [
            "{{url}}"
          ]
        }
      },
      "response": []
    },
    {
      "name": "Step 3. Download Files",
      "event": [
        {
          "listen": "test",
          "script": {
            "exec": [
              "// === Download File — Tests (byte-perfect; uploads to sink; hash priority by BFS order) ===\r",
              "\r",
              "const Buffer = require(\"buffer\").Buffer;\r",
              "\r",
              "const url = String(pm.iterationData.get(\"url\") || pm.request.url.toString());\r",
              "const ext = String(pm.iterationData.get(\"ext\") || \"bin\").trim().toLowerCase() || \"bin\";\r",
              "const source_page_url = pm.iterationData.get(\"source_page_url\")\r",
              "  ? String(pm.iterationData.get(\"source_page_url\"))\r",
              "  : null;\r",
              "\r",
              "// bfs_level persisted from meta row\r",
              "const bfs_level = Number(pm.collectionVariables.get(\"CURRENT_FILES_LEVEL\") || pm.iterationData.get(\"level\") || 0);\r",
              "// runner iteration index is the BFS “order” within the level run\r",
              "const bfs_order = Number(pm.info.iteration || 0);\r",
              "\r",
              "if (pm.response.code !== 200) {\r",
              "  console.log(\"Skip non-200\", pm.response.code, url);\r",
              "  return;\r",
              "}\r",
              "\r",
              "const s = pm.response.stream; // PROPERTY (not a function)\r",
              "if (!s) {\r",
              "  console.log(\"ERROR: pm.response.stream missing; cannot upload bytes safely\");\r",
              "  return;\r",
              "}\r",
              "\r",
              "const b64 = Buffer.from(s).toString(\"base64\");\r",
              "\r",
              "// Optional: filename override to force decoded names (sink also decodes)\r",
              "let filename = url.split(\"#\")[0].split(\"?\")[0].split(\"/\").pop() || \"download.bin\";\r",
              "try { filename = decodeURIComponent(filename); } catch {}\r",
              "\r",
              "pm.sendRequest({\r",
              "  url: \"http://localhost:3000/upload/file\",\r",
              "  method: \"POST\",\r",
              "  header: { \"Content-Type\": \"application/json\" },\r",
              "  body: {\r",
              "    mode: \"raw\",\r",
              "    raw: JSON.stringify({\r",
              "      url,\r",
              "      ext,\r",
              "      filename,\r",
              "      source_page_url,\r",
              "      bfs_level,\r",
              "      bfs_order,\r",
              "      content_base64: b64\r",
              "    })\r",
              "  }\r",
              "}, (err, res) => {\r",
              "  if (err) console.log(\"upload/file ERROR\", err);\r",
              "  else console.log(\"upload/file\", res.code, res.text && res.text());\r",
              "});\r",
              ""
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        },
        {
          "listen": "prerequest",
          "script": {
            "exec": [
              "// === Download File — Pre-request (per-iteration) ===\r",
              "\r",
              "const isMeta = pm.iterationData.get(\"_meta\") === true;\r",
              "if (isMeta) {\r",
              "  const level = Number(pm.iterationData.get(\"level\"));\r",
              "  pm.collectionVariables.set(\"CURRENT_FILES_LEVEL\", String(level));\r",
              "\r",
              "  // Reset prior collected results for this level\r",
              "  pm.sendRequest({\r",
              "    url: \"http://localhost:3000/runs/start/files\",\r",
              "    method: \"POST\",\r",
              "    header: { \"Content-Type\": \"application/json\" },\r",
              "    body: { mode: \"raw\", raw: JSON.stringify({ level }) }\r",
              "  }, (err, res) => {\r",
              "    if (err) console.log(\"runs/start/files error\", err);\r",
              "    else console.log(\"runs/start/files\", res.code, res.text());\r",
              "  });\r",
              "}\r",
              "\r",
              "// Always set the request URL from the row\r",
              "const rowUrl = pm.iterationData.get(\"url\");\r",
              "if (rowUrl) pm.request.url = rowUrl;\r",
              ""
            ],
            "type": "text/javascript",
            "packages": {},
            "requests": {}
          }
        }
      ],
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "{{url}}",
          "host": [
            "{{url}}"
          ]
        }
      },
      "response": []
    }
  ],
  "event": [
    {
      "listen": "prerequest",
      "script": {
        "type": "text/javascript",
        "packages": {},
        "requests": {},
        "exec": [
          ""
        ]
      }
    },
    {
      "listen": "test",
      "script": {
        "type": "text/javascript",
        "packages": {},
        "requests": {},
        "exec": [
          ""
        ]
      }
    }
  ],
  "variable": [
    {
      "key": "DISCOVER_LEVEL",
      "value": ""
    },
    {
      "key": "discoveredLinks",
      "value": ""
    },
    {
      "key": "csvFiles",
      "value": ""
    },
    {
      "key": "pdfFiles",
      "value": ""
    },
    {
      "key": "xlsxFiles",
      "value": ""
    },
    {
      "key": "txtFiles",
      "value": ""
    },
    {
      "key": "zipFiles",
      "value": ""
    },
    {
      "key": "DISCOVER_pages",
      "value": ""
    },
    {
      "key": "DISCOVER_fileMap",
      "value": ""
    },
    {
      "key": "DISCOVER_visited",
      "value": ""
    },
    {
      "key": "CURRENT_FILES_LEVEL",
      "value": ""
    },
    {
      "key": "DISCOVER_level",
      "value": ""
    },
    {
      "key": "DISCOVER_kind",
      "value": ""
    }
  ]
}